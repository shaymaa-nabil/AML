{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_62mxdDlvTa"
      },
      "source": [
        "<table align=\"center\">\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/sherifmost/DeepLearning/blob/master/Labs/lab7/lab7.ipynb\">\n",
        "        <img src=\"http://introtodeeplearning.com/images/colab/colab.png?v2.0\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 4: Transformer-based Sentiment Analysis"
      ],
      "metadata": {
        "id": "UL62wszlXcpA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Transformer Sentiment Analysis](https://github.com/sherifmost/DeepLearning/blob/master/Labs/lab7/Cover.png?raw=1)"
      ],
      "metadata": {
        "id": "HOO67ItIgLZk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Problem Statement"
      ],
      "metadata": {
        "id": "F5MPybf4g15f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this Assignment you will build a transformer encoder **from scratch** to be used as part of a classifier for movie review sentiment analysis on the IMDB dataset.\n",
        "\n",
        "The full classifier consists of the following 3 main components:\n",
        "1. Tokenizer: we will use a readily available BERT tokenizer\n",
        "2. Transformer Encoder: you will implement a transformer encoder from scratch that takes in the outputs of the tokenizer and uses mutli-headed attention blocks and feed forward networks to obtain an output feature representation.\n",
        "3. Classification Head: we will use a fully connected network that takes the output feature representation from the transformer encoder and obtains the output sentiment prediction using sigmoid activation.\n",
        "\n",
        "The IMDB dataset consists of a total of 25,000 training examples and 25,000 testing examples with different sentence lengths for the reviews.\n",
        "\n",
        "We will rely on both quantitative evaluation (using the accuracy metric) and qualitative evaluation (by inspecting the model's output on some test samples and comparing it to the actual output)."
      ],
      "metadata": {
        "id": "4mt-uWmsg4Ew"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6C9ccyyCtq8n"
      },
      "source": [
        "**IMPORTANT NOTE:** You have to change runtime type on Google Colab to GPU since this assignment requires much computation resources and it will run very slowly on CPU (Default runtime type)\n",
        "\n",
        "Click on \"Runtime\" => \"Change runtime type\" => make sure that GPU is selected in the \"Hardware accelerator\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXQOsmZj_OET"
      },
      "source": [
        "Now lets walk through the code, and tell you the parts you need to fill.\n",
        "\n",
        "**MAKE SURE YOU KEEP ALL THE OUTPUTS FOR THE SUBMISSION**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbuTZrb6sEPE"
      },
      "source": [
        "## 4.2 Problem Details"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0xhJ9LCsNjL"
      },
      "source": [
        "### 4.2.1 Installing and Importing the Needed Packages"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: You might need to restart your session after running the following cell. **If prompted to do so**, just click restart session and run the cells again. Otherwise, continue running the cells without restarting."
      ],
      "metadata": {
        "id": "A_pHmg__jnvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Need to install this particular version of tensorflow_text as it allows integrating the BERT tokenizer into the model\n",
        "################################### YOU MIGHT NEED TO RESTART YOUR SESSION AFTER RUNNING THIS CELL ###################################\n",
        "!pip install -U \"tensorflow-text==2.13.*\""
      ],
      "metadata": {
        "id": "S6ewv1tmlmVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The datasets package will be used for loading the IMDB dataset\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "ixUOmGz7h8Zg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UqB08cqEV7f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "# Needed for the tokenizer part of the model\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text\n",
        "from datasets import Dataset, DatasetDict, load_dataset\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxp8lLUi_idH"
      },
      "source": [
        "### 4.2.2 The IMDB Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this assignment, we will use the [IMDB dataset](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews), consisting of 25k training movie reviews and 25k testing movie reviews with a label (0/1) for each review indicating whether it is negative or positive.\n",
        "\n",
        "Let's load the dataset to the session and inspect it."
      ],
      "metadata": {
        "id": "Wl1Ul2zlkSpl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-M6Czq186Vo"
      },
      "source": [
        "#### 4.2.2.1 Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the training and testing datasets\n",
        "dataset_train = load_dataset(\"imdb\", split = \"train\")\n",
        "dataset_test = load_dataset(\"imdb\", split = \"test\")\n",
        "\n",
        "# Convert the training dataset to a temporary dataframe to inspect it\n",
        "# You can use the interactive table option of the dataframe to inspect the dataset as you like\n",
        "pd.DataFrame.from_dict(dataset_train)"
      ],
      "metadata": {
        "id": "ieTFd6PjiOqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTwFx4FbW253"
      },
      "source": [
        "### 4.2.3 The BERT Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization transforms your raw text data by numerically encoding them based on your vocabulary. This allows a transformer model to understand the input text.\n",
        "\n",
        "Here we use the BERT tokenizer to obtain the numerical token corresponding to each word in the text, type id values representing the sentence this token belongs to (in our case we have only one input sentence so we should expect all tokens to have a type id of *zero*), and attention masks that allow us to only include the parts of the sentence that contains valid text.\n",
        "\n",
        "**You can read more about the BERT tokenizer [here](https://www.analyticsvidhya.com/blog/2021/09/an-explanatory-guide-to-bert-tokenizer/).**"
      ],
      "metadata": {
        "id": "LRBc3-iqOP9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtaining the BERT tokenizer from tensorflow_hub\n",
        "tokenizer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\", name = \"tokenizer\")"
      ],
      "metadata": {
        "id": "r27OyEgoj9g-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's Have a look at the tokenizer in action.\n",
        "\n",
        "*Now pause for a second and think about the following:*\n",
        "\n",
        "*   What do the output arrays obtained from the tokenizer: 'input_word_ids', 'input_mask', and 'input_type ids' represent? How will you use them as input to your transformer encoder?\n",
        "* Why do the output arrays have the same number of elements?\n",
        "* What do the first and last elements in the 'input_word_ids' array represent?\n",
        "\n",
        "Note that as this tokenizer is a tensorflow model, we can include it directly as part of our full model regardless of how you implement the transformer encoder."
      ],
      "metadata": {
        "id": "pxdaCYQ7nC0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer([\"this is an amazing movie!\"])"
      ],
      "metadata": {
        "id": "ovhwAU0em8jU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPTESI55qDML"
      },
      "source": [
        "### 4.2.4 The Transformer Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/sherifmost/DeepLearning/blob/master/Labs/lab7/Encoder_Abstract.png?raw=1\" width=\"300\" height=\"500\">"
      ],
      "metadata": {
        "id": "DUt5x4AYwBXy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The main grading criteria for this part is that your implementation correctly maps the transformer encoder architecture and that it works without errors. For the hyperparameters, you can choose any value you like as long as it allows you to get satisfactory testing accuracy at the end without underfitting/overfitting.**"
      ],
      "metadata": {
        "id": "vrdsNXTTuURl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here you will implement your transformer encoder.\n",
        "\n",
        "Your encoder should follow the transformer encoder architecture and should include the following:\n",
        "\n",
        "\n",
        "*   Word Embeddings and Position Embeddings: you can use Tensorflow's [Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) layer (Note that we don't need the token type embedding since our input consists of a single sentence).\n",
        "*   Multiple Consecutive Blocks (**at least 2 blocks**) of:\n",
        "  * Multi-Headed Self-Attention: you can use Tensorflow's [MultiHeadAttention](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention) layer\n",
        "  * Skip Connection and Normalization: you can use Tensorflow's [Add](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Add) and [LayerNormalization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization) layers\n",
        "  * Intermediate Feed-Forward Network: you can use Tensorflow's [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) layer\n",
        "  * Another Skip Connection and Normalization\n",
        "  * [Dropout](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout) layers as needed to avoid overfitting\n",
        "\n",
        "The input of the encoder will be the matrices output by the tokenizer while the output of the encoder will be the output of the feed foward network in the final block.\n",
        "\n",
        "You can check this [diagram](https://raw.githubusercontent.com/gmihaila/ml_things/master/notebooks/pytorch/bert_inner_workings/bert_inner_workings.png) showcasing the full arctiecture of a BERT encoder model. Use it as a guide when building your own transformer encoder, but don't follow its hyperparameters exactly as the BERT encoder takes a long time to train."
      ],
      "metadata": {
        "id": "3IUMvEntqDMS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO:** fill in the missing code to define a transformer encoder model."
      ],
      "metadata": {
        "id": "Hka18qFf3BOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: fill in this function to define a transformer encoder\n",
        "# The input to the encoder should be the outputs of the tokenizer inspected above\n",
        "# The output of the encoder should be a flat feature vector representing the accumulated representation for the features extracted by the encoder\n",
        "\n",
        "# Make sure your encoder follows the typical transformer architecture and uses each of the following imported tensorflow layers\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, LayerNormalization, Dropout, Flatten, MultiHeadAttention, Add\n",
        "\n",
        "# This function should return a tensorflow Model()\n",
        "# Note that relying on the functional approach of building the model (similar to the past assignments) will facilitate the code\n",
        "def get_transformer_encoder(vocab_size = 30522):\n",
        "  # Inputs\n",
        "  input_word_ids = Input(shape=(None,), dtype=tf.int32, name='input_word_ids')\n",
        "  input_mask = Input(shape=(None,), dtype=tf.int32, name='input_mask')\n",
        "  # Position indexes for the input tokens\n",
        "  position_indexes = tf.range(start=0, limit=tf.shape(input_word_ids)[1], delta=1)\n",
        "\n",
        "  # ToDo: Define the Embedding layers\n",
        "  word_embedding_layer = #ToDo\n",
        "  positions_embedding_layer = #ToDo\n",
        "\n",
        "  # Combine embeddings\n",
        "  embeddings = word_embedding_layer(input_word_ids) + positions_embedding_layer(position_indexes)\n",
        "\n",
        "  # ToDo: Implement Transformer attention and feedforward blocks given the created embeddings.\n",
        "  # Use the input_mask to mask the attention operation. Add dropout regularization as needed to handle overfitting.\n",
        "  # Implement at least 2 consecutive attention and feedforward blocks.\n",
        "\n",
        "  # First Block\n",
        "  block_output1 = #ToDo\n",
        "\n",
        "  # Second Block\n",
        "  block_output2 = #ToDo\n",
        "\n",
        "  # Add more blocks if needed to handle underfitting\n",
        "\n",
        "  # ToDo: add here the output of the feedforward network of the last block\n",
        "  blocks_output = #ToDo\n",
        "\n",
        "  # The encoder output is a layer normalization on the last feed forward network output\n",
        "  encoder_output = LayerNormalization(epsilon=1e-6)(blocks_output)\n",
        "  encoder_output = Flatten()(encoder_output)\n",
        "\n",
        "  return Model(inputs=[input_word_ids, input_mask], outputs=encoder_output)"
      ],
      "metadata": {
        "id": "bNPlhe3mpl3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8rqWtQSyZuH"
      },
      "source": [
        "### 4.2.5 The Full Classifier Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's define the full classification model by combining the tokenizer with your implemented transformer encoder model and adding a classification head to the encoder's output."
      ],
      "metadata": {
        "id": "o_ciPu2Ayclz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_classifier_model():\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  tokenizer_output = tokenizer(text_input)\n",
        "  encoder_inputs = {\n",
        "      'input_word_ids': tokenizer_output['input_word_ids'],\n",
        "      'input_mask': tokenizer_output['input_mask'],\n",
        "  }\n",
        "\n",
        "  # Use here your defined encoder that takes the output of the tokenizer\n",
        "  encoder = get_transformer_encoder()\n",
        "  encoder_outputs = encoder(encoder_inputs)\n",
        "\n",
        "  output = tf.keras.layers.Dropout(0.2)(encoder_outputs)\n",
        "  output = tf.keras.layers.Dense(1, activation=None, name='classification_output')(output)\n",
        "  return tf.keras.Model(text_input, output)"
      ],
      "metadata": {
        "id": "cUQzp80VqZaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Take a look at your model's summary. Note the number of parameters and their size in MBs (transformers are large models and require extensive resources for training and storage)."
      ],
      "metadata": {
        "id": "hgGQVc_qy5VD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_classifier_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "79kIOvh5tavt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take a look at the model's structure as a diagram."
      ],
      "metadata": {
        "id": "HunwhrHlzAtc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.utils.plot_model(model, show_shapes=True, expand_nested=True)"
      ],
      "metadata": {
        "id": "kuXWUN5QyMic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "au-4innlzNx1"
      },
      "source": [
        "### 4.2.6 Training the Model and Performing Quantitative Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following function helps us plot the training and validation accuracy and loss after training."
      ],
      "metadata": {
        "id": "ClOTGzeD0E08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the training history\n",
        "def plot_train_history(hist,\n",
        "                       metric = 'accuracy'):\n",
        "\n",
        "  fig = plt.figure(figsize=(10, 5))\n",
        "  # Get training and test loss histories\n",
        "  trainingLoss = hist.history['loss']\n",
        "  valLoss = hist.history['val_loss']\n",
        "\n",
        "  # Create count of the number of epochs\n",
        "  epochCount = range(1, len(trainingLoss) + 1)\n",
        "\n",
        "  # Visualize loss history\n",
        "  fig.add_subplot(1,2,1)\n",
        "  plt.plot(epochCount, trainingLoss, 'r--')\n",
        "  plt.plot(epochCount, valLoss, 'b-')\n",
        "  plt.legend(['Training Loss', 'Val Loss'])\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "\n",
        "  # Get training and test accuracy histories\n",
        "  trainingAcc = hist.history[metric]\n",
        "  valAcc = hist.history['val_' + metric]\n",
        "\n",
        "  # Create count of the number of epochs\n",
        "  epoch_count = range(1, len(trainingAcc) + 1)\n",
        "\n",
        "  # Visualize accuracy history\n",
        "  fig.add_subplot(1,2,2)\n",
        "  plt.plot(epoch_count, trainingAcc, 'r--')\n",
        "  plt.plot(epoch_count, valAcc, 'b-')\n",
        "  plt.legend(['Training Accuracy', 'Validation Accuracy'])\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Accuracy')"
      ],
      "metadata": {
        "id": "gnA2CEN7GY2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.2.6.1 Defining the Training Hyperparameters"
      ],
      "metadata": {
        "id": "dg404KfW2bab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the loss function and the evaluation metric\n",
        "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "metrics = tf.metrics.BinaryAccuracy()"
      ],
      "metadata": {
        "id": "EG-g3Upv2cqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You are encouraged to experiment with tuning the following hyperparameters to handle overfitting/underfitting\n",
        "epochs = 15\n",
        "batch_size = 32\n",
        "lr = 3e-5\n",
        "optimizer = tf.keras.optimizers.AdamW(learning_rate=lr)"
      ],
      "metadata": {
        "id": "Y-K2n7Ho2p-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.2.6.2 Compiling and Training the Model"
      ],
      "metadata": {
        "id": "dgtYH2Dzz-9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling the model using the loss and evaluation metrics\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=loss,\n",
        "              metrics=metrics)"
      ],
      "metadata": {
        "id": "4EbNbNZxAy1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO:** Make sure to handle any underfitting (*training accuracy should at least be more than 90%*) or overfitting in the training. You can try early stopping and/or regularization methods."
      ],
      "metadata": {
        "id": "f_-kD3qaesg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ToDo: Make sure to handle overfitting/underfitting\n",
        "# Running the training and obtaining a plot for it\n",
        "history = model.fit(dataset_train['text'],\n",
        "                    dataset_train['label'],\n",
        "                    validation_data = (dataset_test['text'], dataset_test['label']),\n",
        "                    epochs=epochs,\n",
        "                    batch_size=batch_size)\n",
        "\n",
        "plot_train_history(history, metric = 'binary_accuracy')"
      ],
      "metadata": {
        "id": "mKePRkL8FYTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHE5ETCy0UDY"
      },
      "source": [
        "### 4.2.7 Qualitative Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This function converts the output model probabiltiy into a sentiment (Positive or Negative)\n",
        "def get_sentiment(probability_positive):\n",
        "  if probability_positive > 0.5:\n",
        "    return \"Positive\"\n",
        "  else:\n",
        "    return \"Negative\""
      ],
      "metadata": {
        "id": "8tlHN6L8DOoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check the model's output compared to random review samples from the testing data. You can run the following cell multiple times to see different examples!"
      ],
      "metadata": {
        "id": "6B-2LiAo0c68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomly select a test review and check the model's output on it\n",
        "# You can run it multiple times to check different samples\n",
        "# After running this cell, keep your output\n",
        "for i in range(10):\n",
        "  random_id = np.random.randint(0, len(dataset_test))\n",
        "  test_review = dataset_test['text'][random_id]\n",
        "  test_label = dataset_test['label'][random_id]\n",
        "  print(\"Review: \", test_review)\n",
        "  print(\"Ground Truth Sentiment: \", get_sentiment(test_label))\n",
        "  # Prediction probability of the Positive review, i.e., 1 using sigmoid function:\n",
        "  probability_positive = 1/(1 + np.exp(-model.predict([test_review], verbose = 0)[0][0]))\n",
        "  probability_negative = 1 - probability_positive\n",
        "  print(\"Predicted Sentiment: \", get_sentiment(probability_positive))\n",
        "  print(\"Prediction Probability: Positive({}), Negative({})\".format(probability_positive, probability_negative))\n",
        "  print(\"-\"*200)"
      ],
      "metadata": {
        "id": "KvB4A-fwA5bR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1NPM797C_5V"
      },
      "source": [
        "## 4.3 Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPhiXDFhrim8"
      },
      "source": [
        "That's it! Congratulations on training a transformer-based sentiment analysis model.\n",
        "\n",
        "Make sure you deliver all the requirements for the submission and to keep the outputs in the notebook!"
      ]
    }
  ]
}